# 常用激活函数及其导数
- Sigmoid:
  $$f(z)=\frac{1}{1+exp(-z)}, f'(z)=f(z)(1-f(z))$$
- Tanh:
  $$f(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}, f'(z)=1-f^2(z)$$
 ReLU:
  $$f(z)=\max(0,z)$$

# 为什么sigmoid和tanh会导致梯度消失？
- sigmoid在 z 很大的时候趋近于1， z 很小的时候趋近于0.造成梯度消失
- $tanh(x)=2sigmoid(2x)-1$，相当于sigmoid的平移

# ReLU系列的激活函数相对于Sigmoid和Tanh的优点及局限性分别是什么？
- 优点
  - 计算上ReLU更简便
  - ReLU非饱和，可以有效解决梯度消失问题
  - ReLU的单侧抑制提高了网络的稀疏表达能力。(即神经元只在处理相关事物时被激活，加快计算速度)
- 局限性
  - 如果学习率设置较大，可能导致超过一定比例的神经元不可逆死亡，进而整个训练失败。原因是流经这些神经元的梯度输入永远为负，输出永远为0。Leaky ReLU在一定程度上解决这一问题

# 神经网络训练技巧
## 神经网络训练时是否可以将全部参数初始化为0
对于全连接的深度神经网络，同一层中的神经元都是同构的。如果将参数初始化为同样的值，那么无论前向传播还是反向传播的取值都是完全相同的。学习过程将无法打破这种对称性，最终同一网络层中的各个参数仍然是相同的。

## 为什么Dropout可以抑制过拟合？它的工作原理和实现？
- Dropout作用于每份小批量数据上，以一定的概率临时丢弃一部分神经元节点。
- 对于一个包含N个神经元节点的网络，在Dropout的作用下可看作$2^N$个模型的集成。这$2^N$个模型共享部分权值，且拥有相同的网络参数，大大简化了运算。
- 这个过程会减弱全体神经元之间的联合适应性，减少过拟合风险，增强泛化能力。
- 训练过程中以概率$p$随机保留神经元。
- 测试过程中不随机丢弃神经元，但要将每个神经元的参数诚意概率系数$p$，以恢复训练中该神经元只有$p$的概率被用于前向传播

## 批量归一化的基本动机与原理
- 尽管我们通常已经将输入归一化处理，但随着网络训练的进行，每层的参数变化使得后一层的输入分布发生改变。因此对于不同的batch，每层网络需要应对不同的数据分布，增大了训练的复杂度。
- Batch Normalization是指针对每一批数据，在网络的每一层输入之前增加归一化处理(均值为$\beta$，标准差为$\gamma$)。该层第k个神经元输入$\hat x_k$可以表示为
  $$\hat x_k=\gamma_k\frac{x_k-E[x_k]}{\sqrt{Var[x_k]}}+\beta_k$$
  - $E[x_k]$指这一个batch数据在第k维的均值
  - $\sqrt{Var[x_k]}$指这一个batch数据在第k维的标准差
  - $\beta_k$和$\gamma_k$均为可学习参数，且针对第k个神经元。也就是说在BN1d中有多少个神经元就有多少个$(\gamma, \beta)$对。
- 因为实现了输入数据分布与之前网络层的解耦，BN可以一定程度上减轻梯度消失和梯度爆炸

## 批量归一化在卷积神经网络的应用
每一个卷积核的参数在不同位置的神经元当中是共享的，因此也应该被一起归一化。如果有f个卷积核，就对应f个特征图和f组不同的$\gamma,\beta$参数。

详细来说：
- BN2d首先将输入数据在**Batch, 宽， 高**三个方向上归一化。也就是说对于一个batch，每个channel对应一对均值，方差。
- 对每个channel学习一对$(\gamma,\beta)$

## 预测时如何使用BN
- 计算输入的均值和方差
- 将输入沿batch方向归一化
- 使用学习到的$(\gamma,\beta)$对进行分布优化。

# 深度卷积神经网络
## 卷积操作的本质包括稀疏交互和参数共享
- 稀疏交互(sparse interaction)：每个输出神经元仅与前一层特定区域内的神经元存在交互。这样大大减少了参数数量且改善过拟合现象。
  - 物理意义：现实世界中的数据都具有局部的特征结构。因此可以先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征。
  - 以人脸识别为例：最底层的神经元检测出边缘特征；中间的神经元将边缘组合起来得到眼睛、鼻子、嘴巴等复杂特征；最后，位于上层的神经元可以根据器官的组合检测出人脸的特征。(如何将特征图可视化见[Visualizing and Understanding Convolutional Networks](https://zhuanlan.zhihu.com/p/36348924))

- 参数共享(Parameter sharing)
  - 参数共享是卷积运算的固有属性。
  - 参数共享的物理意义是使得卷积层的输出**对于平移变换是等变的**，即卷积后平移与平移后卷积的输出结果是相等的。因此假如图像中有一只猫，那么无论它出现在图像中的任何位置，都可以将其识别为猫。

## 常见的池化操作
- 均值池化：抑制由于卷积层邻域大小受限造成的估计值方差增大，更好地保留背景。
- 最大池化：抑制网络参数误差造成的估计均值偏移(关注的不是重点)，可以更好的提取纹理信息。
- 池化层能显著降低参数量，并保持**对平移、伸缩、旋转操作的不变性**。

## 卷积神经网络如何用于文本分类任务

# 深度残差网络
- 目的：解决或缓解深层神经网路训练中的梯度消失问题。