# 支持向量机
## hard-margin SVM
- 模型：$f(\underline x)=sign(\underline w^T\underline x + b)$
- 策略：最大化支持向量到超平面的距离
$$\underline \theta^*=\max_{\underline \theta}\min_{i}\frac{1}{||\underline w||}y_i(\underline w^T\underline x_i+b)\ s.t.\ y_i(\underline w^T\underline x_i+b)\geq0$$

由于$\underline w, b$可以等比例缩放，我们假设支持向量满足$\underline w^T\underline x_i+b=1$

- 目标函数：
$$\underline \theta^*=\argmin_{\underline \theta}J(\underline \theta),\ J(\underline \theta)=\frac{1}{2}\underline w^T\underline w,\ s.t.\ y_i(\underline w_i^T\underline x_i+b)\geq1$$

- 解约束优化问题：
$$\min J(\underline \theta),\ s.t.\ c_i(\underline \theta)\geq0$$
KKT conditons:

$L(\underline \theta, \underline \lambda)=J(\underline \theta)-\sum_i\lambda_ic_i(\underline \theta)=\frac{1}{2}\underline w^T\underline w-\sum_i\lambda_i (y_i(\underline w_i^T\underline x_i+b)-1)$

$\nabla_{\underline w}L(\underline \theta, \lambda)=\underline w-\sum_i\lambda_iy_i\underline x_i$

$\nabla_{b}L(\underline \theta, \lambda)=-\sum_i\lambda_iy_i$

$$\nabla_{\underline \theta}L(\underline \theta^*, \lambda)=\underline 0\Leftrightarrow \underline w^* = \sum_i\lambda_iy_i\underline x_i,\ \sum_i\lambda_iy_i=0$$
又有
$$\lambda_i^*c_i(\underline \theta)=0, c_i(\underline \theta^*)\geq0$$
故对于非支持向量($c_i(\underline \theta)\geq0$)必有$\lambda^*_i=0$，即非支持向量不影响$\underline w^* = \sum_i\lambda_iy_i\underline x_i$

- 结论：仅支持向量影响分类结果。$\underline w^* = \sum_i\lambda_iy_i\underline x_i$，$b$一般任意找出一个支持向量$$(\underline x_k,y_k)并使用$b^*=y_k-\sum_i\lambda_iy_i\underline x_i^T\underline x_k$

## soft-margin SVM
- 模型：$f(\underline x)=sign(\underline w^T\underline x + b)$
- 目标函数：松弛变量允许某些点跨越边界，但同时使用正则化尽可能缩小$\xi_i$
$$\underline \theta^*=\argmin_{\underline \theta}J(\underline \theta),\ J(\underline \theta)=\frac{1}{2}\underline w^T\underline w+c\sum_i^N\xi_i,\ s.t.\ y_i(\underline w_i^T\underline x_i+b)\geq1-\xi_i,\xi_i\geq0$$

## 核方法
- 模型：$f(\underline x)=sign(\underline w^T\underline\phi(\underline x) + b)$
- kernlel trick: 某些函数可以实现$K(\underline x_i, \underline x_j)=\underline \phi(\underline x_i)^T\underline \phi(\underline x_j)$，其中$\underline \phi(\underline x)$可能无限维。
- 推理：$f(\underline x)=sign(\underline w^T\underline\phi(\underline x) + b)=sign(\sum_i\lambda_iy_iK(\underline x_i,\underline x) + b)$
其中$\underline x_i$为训练集中的支持向量，$\underline x$为新数据。
- 高斯核
$$K(\underline x,\underline z)=\exp(-\frac{||\underline x-\underline z||^2}{2\sigma^2})$$

## 细节
- 对于任意线性可分的两组点，它们在SVM分类的超平面上都是**线性不可分的**
- 由于高斯核可以实现到无限维特征空间的映射，不考虑松弛变量的SVM一定可以实现训练误差为0(所有训练集数据都被正确分类)

# 逻辑回归
- 模型：$P(y=1|\underline x)=\sigma(\underline w^T\underline x)$
- 策略: MLE，考虑$y\in\{0,1\}$, loglikelihood为
$$y\log[\sigma(\underline w^T\underline x)]+(1-y)\log[1-\sigma(\underline w^T\underline x)]$$

## 细节
- 每个样本仅对应于一个标签的多分类问题：softmax regression
- 存在样本属于多个标签的多分类问题：对于每一个可能的类别训练一个属于/不属于该类的二分类器

# 决策树
## 如何构建一个决策树
- ID3：最大信息增益
  - 对于样本集合$D$，类别数为$K$，$C_k$表示样本集合中属于第$k$类的样本子集，则该数据集$D$的**经验熵**为
    $$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$$
  - 假设某个特征$A$有$n$个取值，取第$i$个值的样本子集为$D_i$，则该特征对数据集$D$的**经验条件熵**
    $$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)$$
  - 两者之差表示如果把该特征作为根节点所带来的信息增益多少
    $$g(D,A)=H(D)-H(D|A)$$
  - **信息增益意味着给定条件后不确定性减少的程度，容易倾向取值较多的特征，故泛化能力较弱**
  - **ID3只能处理离散型变量，只能用于分类任务**

- C4.5: 最大信息增益比
  - 选择信息增益比$g_R(D,A)$最大的特征作为根节点
    $$g_R(D,A)=\frac{g(D,A)}{H_A(D)},\ H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$$
  - $H_A(D)$称为数据集$D$关于$A$的**取值熵**

- CART： 最小基尼指数
  - 选择基尼指数最小的特征及切分点将数据按照**二叉树**的方式分割。
    $$Gini(D|A=A_i)=\frac{|D_{A=A_i}|}{|D|}Gini(D_{A=A_i})+\frac{|D_{A\neq A_i}|}{|D|}Gini(D_{A\neq A_i}),\ Gini(D)=1-\sum_{k=1}^n\left(\frac{|C_k|}{|D|}\right)^2$$
  - 上式中的$A=A_i$也可以根据特诊离散/连续换成$A<A_i$等
  - CART既可用于分类也可用于回归(使用最小平方误差)。
  - 推理时，找到新数据从属的叶节点，vote/求均值。

## 如何剪枝
一棵完全生长的决策树会面临过拟合问题，即每个叶节点仅包含一个样本。为了提高决策树的泛化能力，我们需要进行剪枝。
- 预剪枝：在生成决策树的过程中提前停止树的增长
  - 当树到达一定深度时，停止树的生长
  - 当到达当前节点的样本数量小于某个阈值时，停止树的生长
  - 计算每次分裂对测试集准确度的提升，当提升小于某个阈值时，不再继续扩展
- 后剪枝：在已生成的过拟合决策树上剪去分支，使得非叶节点变成叶节点
  - 每次剪去使得训练数据集合误差增加最小的分支(非叶节点)
  - 完整决策树记为$T_0$，$T_i$剪去一个分支得到$T_{i+1}$。假设完整决策树有$n$个节点，则可得到子树序列$[T_0,T_1,...,T_n]$
  - 从子树序列$[T_0,T_1,...,T_n]$中选取真实误差最小的决策树：使用k-fold，将训练集分成$k$份，前$k-1$份用于生成决策树，最后一份用于选择最优的剪枝树。


