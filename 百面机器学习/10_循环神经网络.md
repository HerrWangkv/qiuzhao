# 循环神经网络和卷积神经网络
- 卷积神经网络对文本数据建模时，可以捕捉到原文本中的一些局部特征，但两个单词之间的长距离依赖关系还是很难被学习到。
- 循环神经网络从前到后阅读文章中的每一个单词，将前面阅读到的有用信息编码到状态变量中，最终的输出$y$也仅与最后一个frame的状态相关。
  $$net_t=W_xx_t+W_hh_{t-1},\ h_t=f(net_t),\ y=g(Vh_T)$$

# 循环神经网络的梯度消失和梯度爆炸问题
$$net_t=W_xx_t+W_hf(net_{t-1}),\ \frac{\partial net_{t}}{\partial net_{t-1}}=W_h\cdot diag[f'(net_{t-1})]$$
$$\frac{\partial y}{\partial net_t}\propto \frac{\partial net_T}{\partial net_{T-1}}\cdot...\cdot \frac{\partial net_{t+1}}{\partial net_{t}}$$
- 由于预测误差是沿着神经网络的每一层反向传播的(数学上理解为多个雅克比矩阵的乘积)，当雅克比矩阵的最大特征值大于1时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸。
- 若雅克比矩阵的最大特征值小于1，梯度的大小会呈指数缩小，产生梯度消失。
- 梯度爆炸的问题可以通过gradient clipping 解决
- 梯度消失必须通过改变模型本身来解决。可使用LSTM或GRU

# 循环神经网络中的激活函数
- 卷积神经网络等前馈神经网络采用ReLU可以有效改善梯度消失(因为ReLU不存在梯度饱和现象且各层的参数之间是独立分布的)
- 对于循环神经网络，即使采用了ReLU激活函数，但由于共享参数$W_h$，只要$W_h$不是单位矩阵，还是会出现梯度消失或梯度爆炸的现象。
- 如果想要使用ReLU作为RNN的激活函数时，只有当$W_h$的取值在单位矩阵附近时才能取得较好的效果。一般将$W_h$直接初始化为单位矩阵。

# 长短期记忆网络
- 输入门 $i_t=\sigma(W_ix_t+U_ih_{t-1}+b_i)$
- 遗忘门 $f_t=\sigma(W_fx_t+U_fh_{t-1}+b_f)$
- 输出门 $o_t=\sigma(W_ox_t+U_oh_{t-1}+b_o)$
- 短期候选记忆 $\tilde c_t=\tanh(W_cx_t+U_ch_{t-1})$
- 长期记忆 $c_t=f_t\odot c_{t-1}+i_t \odot\tilde c_t$
- $h_t=o_t\odot\tanh(c_t)$

## LSTM各模块的激活函数可否替换成其他激活函数
- 三个门函数都使用sigmoid函数，保证输出在$(0,1)$之间符合门控的物理意义。
- 生成候选记忆时，使用Tanh函数，使得输出在$(-1,1)$内，与大多数场景中特征分布是0中心相吻合，且在0附近梯度更大，使模型更快收敛。

# Seq2Seq模型

# 注意力机制