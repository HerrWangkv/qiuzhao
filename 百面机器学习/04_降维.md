# PCA 主成分分析(最大方差角度)
- 策略：最大化投影方差
- 原理
  - 中心化得到$\{\underline x_1,\underline x_2,...,\underline x_n\}$
  - 在单位向量$\underline w$上投影得$\underline w^T\underline x$
  - 投影后的方差为
    $$D(\underline X)=\frac{1}{n}\sum_{i=1}^n\underline w^T\underline x_i\underline x_i^T\underline w=\underline w^T\underline \Sigma \underline w$$
    其中$\underline \Sigma=\frac{1}{n}\underline x_i\underline x_i^T$为随机变量$\underline X$的协方差矩阵
  - 问题变成一个约束优化问题：
    $$\max\ \underline w^T\underline \Sigma\underline w,\ s.t.\ \underline w^T\underline w=1$$
  - 引入拉格朗日乘子:
    $$L(\underline w, \lambda)=\underline w^T\underline \Sigma\underline w-\lambda\underline w^T\underline w+\lambda$$
    $$\nabla_wL(\underline w^*, \lambda^*) = 2\underline \Sigma\underline w^*-2\lambda\underline w^*=\underline 0$$
  - 结论
    - 最优投影向量应为协方差矩阵$\underline \Sigma=\frac{1}{n}\underline x_i\underline x_i^T$的特征向量。
    - 对应的特征值 $\lambda$ 越大, 方差 $\underline w^T\underline \Sigma\underline w=\lambda\underline w^T\underline w$ 越大

- 步骤
  - 中心化
  - 求样本协方差矩阵$\underline \Sigma=\frac{1}{n}\underline x_i\underline x_i^T$
  - 对协方差矩阵进行特征值分解，将特征值从大到小排列
  - 取特征值前 d 大对应的特征向量$\{\underline w_1, \underline w_2,...,\underline w_d\}$，将样本映射到该d维得$[\underline w_1^T\underline x,...,\underline w_d\underline x]^T$。在n维空间中重建可得$\underline{\tilde x}=\sum_{i=1}^d(\underline w_i^T\underline x)\underline w_i$

# PCA 最小平方误差理论
- 找到一个d维超平面，使得数据点到这个超平面的距离平方和最小。
- 假设该超平面由d个标准正交基$\underline W=\{\underline w_1,\underline w_2,...,\underline w_d\}$
- 重建为$\underline{\tilde x}=\sum_{i=1}^d(\underline w_i^T\underline x)\underline w_i$
- 目标函数:
    $$\min_{\underline w_1,\underline w_2,...,\underline w_d}\sum_{k=1}^n||\underline x_k-\underline{\tilde x}_k||^2,\ s.t.\ \underline w_i^T\underline w_j=\delta_{ij}$$
- 推导过程与最大方差角度相同

# LDA 线性判别分析
- 思想：最大化类间距离，最小化类内距离
- 设$C1,C2$两类的期望点分别为 $\underline \mu_1$ 和 $\underline \mu_2$，投影轴单位向量为 $\underline w$
- 目标函数:
    $$\max_w J(\underline w)=\frac{\underline w^T(\underline \mu_1-\underline \mu_2)(\underline \mu_1-\underline \mu_2)^T\underline w}{D_1+D_2},\ D_1=\sum_{x\in C_1}\underline w^T(\underline x-\underline \mu_1)(\underline x-\underline \mu_1)^T\underline w$$
    $D_1,D_2$分别为两类点投影后的方差
- 简化：
  - 类间散度矩阵 $S_B=(\underline \mu_1-\underline \mu_2)(\underline \mu_1-\underline \mu_2)^T$
  - 类内散度矩阵 $S_w=\sum_i\sum_{x\in C_i}(\underline x-\underline \mu_i)(\underline x-\underline \mu_i)^T$
  - 目标函数 
    $$J(\underline w)=\frac{\underline w^T\underline S_B\underline w}{\underline w^T\underline S_w\underline w}$$
- 求导得
    $$\underline S_w^{-1}\underline S_B\underline w=\lambda\underline w,\ \lambda=J(\underline w)$$
  故求解 $\underline S_w^{-1}\underline S_B$ 最大特征值对应的特征向量即为LDA投影方向
- 将LDA扩展到多类：
  - 类内散度矩阵保持不变：$S_w=\sum_i\sum_{x\in C_i}(\underline x-\underline \mu_i)(\underline x-\underline \mu_i)^T$
  - 全局散度: $S_t=\sum_{i=1}^n(\underline x_i-\underline \mu)(\underline x_i-\underline \mu)^T$
  - 类间矩阵：$\underline S_b=\underline S_t-\underline S_w$
- 将LDA扩展到多类高维(即不止降一维，从$\underline x$降到$\underline W^T\underline x,\ \underline W=[\underline w_1,...,\underline w_d]$)
  - 目标函数:
    $$J(\underline W)=\frac{tr(\underline W^T\underline S_B\underline W)}{tr(\underline W^T\underline S_w\underline W)}$$
# LDA vs. PCA
对无监督的任务使用PCA，对有监督的任务选择LDA