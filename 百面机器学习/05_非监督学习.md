# K均值聚类 K-Means
- 预处理：归一化
- 随机选取 K 个簇中心，记为$\underline\mu_1^{(0)},\underline\mu_2^{(0)},..., \underline\mu_K^{(0)}$
- 定义代价函数
    $$J(c,\mu)=\min_\mu\min_c\sum_{i=1}^M||\underline{x_i}-\underline{\mu}_{c_i}||^2$$
    其中 M 为样本总量，$c_i$是样本$\underline{x}_i$所属于的簇，$\underline{\mu}_i$是$c_i$的中心点。
- 实操
  - 对每一个样本$\underline x_i$，将其分配到距离最近的簇
    $$c_i^{(t)}=\argmin_k||\underline x_i-\underline\mu_k^{(t)}||^2$$
  - 对每一个类簇 k, 重新计算中心
    $$\underline\mu_k^{(t+1)}=\argmin_\mu\sum_{i:c_I{(t)}=k}||\underline x_i-\underline\mu||^2$$

# K-Means的缺点及调优
缺点：
  - 受初值和离群点的影响，每次结果不稳定
  - 无法解决一类是另一类样本100倍的问题。

调优：
  - 预处理：数据归一化和离群点处理。K-Means本质上基于欧氏距离，必须先归一化。
  - 合理选择 K 值，使用手肘法(找出代价函数曲线的拐点)
  - 采用核函数以避免先入为主地认为数据呈球形分布

# 证明 K-Means 算法的收敛性
K-Means本质上是一种最大期望算法(EM算法)，解决的是在概率模型中含有无法观测的隐变量情况下的参数估计问题。
- 一般情况下的最大似然可以写为
  $$\underline{\theta}=\argmax_\theta\sum_{i=1}^N\log P(\underline x_i|\underline \theta)$$
- 含有无法被观测的隐变量时：
  $$\underline{\theta}=\argmax_\theta\sum_{i=1}^N\log\sum_{\underline z_i}Q_i(\underline z_i)P(\underline x_i|\underline z_i,\underline\theta)\\=\argmax_\theta\sum_{i=1}^N\log\sum_{\underline z_i}Q_i(\underline z_i)\frac{P(\underline x_i,\underline z_i|\underline\theta)}{Q_i(\underline z_i)}$$
  $Q_i(\underline z_i)$指对于样本 i 来说，其隐含变量$\underline z_i$的分布。根据Jensen不等式：
  $$\sum_{i=1}^N\log\sum_{\underline z_i}Q_i(\underline z_i)\frac{P(\underline x_i,\underline z_i|\underline\theta)}{Q_i(\underline z_i)}\geq \sum_{i=1}^N\sum_{\underline z_i}Q_i(\underline z_i)\log \frac{P(\underline x_i,\underline z_i|\underline\theta)}{Q_i(\underline z_i)}$$
  等号当且仅当$\frac{P(\underline x_i,\underline z_i|\underline\theta)}{Q_i(\underline z_i)}=c$为常数时成立，又因为$\sum_{\underline z_i}Q_i(\underline z_i)=1$，可得
  $$\frac{1}{c}\sum_{\underline z_i}P(\underline x_i,\underline z_i|\underline\theta)=1, Q_i(\underline z_i)=P(\underline z_i|\underline x_i,\underline\theta)$$
- E-step 使等号成立
  $$Q_i(\underline z_i)=P(\underline z_i|\underline x_i,\underline\theta)$$
- M-step 提高下界
  $$\max_\theta \sum_{i=1}^N\sum_{\underline z_i}Q_i(\underline z_i)\log \frac{P(\underline x_i,\underline z_i|\underline\theta)}{Q_i(\underline z_i)}$$
- 根据单调有界必收敛定理得EM算法是收敛的
- K-Means中
  - $z_i$表示样本 i 从属的簇
  - $P(\underline x_i|z_i,\underline\theta)$与距离成反比
  - $P(z_i|\underline x_i,\underline\theta)$仅当$\underline \mu_{z_i}$是距离样本最近的中心时才为1，反之为0


# GMM 高斯混合模型
- 假设数据呈现的分布是若干个符合高斯分布的簇叠加在一起的结果。
  $$p(\underline{x})=\sum_{i=1}^K\pi_iN(\underline x|\underline\mu_i,\underline\Sigma_i)$$
- 同样使用EM算法迭代求解
  - $z_i$表示样本 i 可能从属的高斯簇
  - $P(\underline x_i|z_i,\underline\theta)=N(\underline x|\underline\mu_{z_i},\underline\Sigma_{z_i})$
  - $P(z_i|\underline x_i,\underline\theta)=\pi_{z_i}$

- GMM 与 K-Means 都只能收敛于局部最优，但GMM还可以给出一个样本属于某类的概率是多少。

# SOM 自组织映射神经网络
通过竞争学习，相邻的神经元(输出)中，仅一个可以被激活(从属的簇)，其他神经元都会被抑制。

# 聚类算法的评估
- R-square: 聚类前后对应的平方误差改进幅度
  $$R^2=\frac{\sum_{\underline x\in D}||\underline x-\underline c||^2-\sum_i\sum_{\underline x\in C_i}||\underline x-\underline c_i||^2}{\sum_{\underline x\in D}||\underline x-\underline c||^2}$$